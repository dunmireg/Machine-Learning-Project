---
title: "Practical Machine Learning Project"
author: "Glenn Dunmire"
date: "Thursday, August 21, 2014"
output: html_document
---

##Background

There are now numerous devices that can be used to collect large amounts of data on personal activity, such as Jawbone Up. Generally, people quanitfy how much of something they do but often do not quantify how well they do it. In this project, I will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly. 

For more information see the Coursera Practical Machine Learning page and and the Human Activity Recognition page found http://groupware.les.inf.puc-rio.br/har.

##Executive Summary

This data comes from http://groupware.les.inf.puc-rio.br/har and will be used to predict the classe variable. I will first clean the data, as there are many columns that are not useful ultimately winding up with 53 columns, meaning 52 variables used to predict classe. I will then fit a random forest model and report the expected out of sample error. 

##Load packages for project

This project depends on the following packages so I will load them now 

```{r}
library(caret)
library(randomForest)
```

##Loading the data

Once navigated into the appropriate directory I will download the two files from the course website into the appropriate directory. 

I will call the files "testing" and "training" respectively. Note I am downloading using URL "http" and NOT "https" because I am using a Windows machine

```{r}
if(!file.exists("training.csv")) {
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training.csv")
}

if(!file.exists("testing.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing.csv")
}
```

The testing data will be used later for the submission portion of the assignment.

Now I'll read the training data into R and take a look at it.

##Examining and Cleaning Data

Examining the document in Excel I noticed a lot of columns are just NA or blank, so I'll tell R to treat "NA" and "" as NAs. 

```{r}
training <- read.csv("training.csv", na.strings = c("NA", ""))
```

So I've got 19,622 observations on 160 variables. That seems like an awful lot. 

```{r}
table(colSums(is.na(training)))
```

I can see I've got 100 columns where the vast majority of strings are NA. I'm going to take those out in the following lines. I'm also going to remove the first 7 columns, as those data don't see relevant to predicting the classe variable (for example this includes a timestamp). Note: I chose to leave the new_window="yes" rows in the data set, although the testing set only has new_window="no". These rows are substanially different but I did not see enough information in the experiment design to make me feel comfortable removing them. 

```{r}
cleanTrain <- training[!colSums(is.na(training)) >= 19216]
cleanTrain <- cleanTrain[,8:60]
```

Now I am left with 52 variables to predict the classe variable, meaning a total of 53 columns. 

##Building the Model

First off I'm going to make a partition, putting 60% of the data into a training set and 40% into a validation set (note: this is all coming from the original training.csv file so this test set is distinct from the testing.csv file).

```{r}
crossVal <- createDataPartition(y = cleanTrain$classe, p = 0.60, list = FALSE)
train <- cleanTrain[crossVal,]
val <- cleanTrain[-crossVal,]
```

Now I'm going to fit a randomForest model, using the randomForest() function from the randomForest package. I chose to use this instead of the method="rf" option in the train() function from caret because this function runs significantly faster and gives very good results. 

```{r}
modFit <- randomForest(classe ~ ., data = train)
modFit
OOB <- modFit$err.rate[modFit$ntree, "OOB"]
```

Officially with random Forests there is no need for cross validation (see http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) as the out of bag error is tested automatically. In my model, the OOB error is `r OOB * 100`% which is very good (meaning accuracy of `r 100 - OOB*100`%)

However, just to be safe I'm going to test my model on my validation set and see how that performs

```{r}
pred <- predict(modFit, val)
conMat <- confusionMatrix(pred, val$classe)
conMat
```

Fantastic! On the validation set I can see my model was `r conMat$overall[[1]] * 100`%, which is in line with what I would expect based on the OOB estimate obtained in the model. The result is also statistically signficant demonstrated by the low p-value. 

Overall I believe this is a good model to estimate the classe variable from this data set. 

##Submission

For the submission portion of the assignment I created a sub directory and used my model to fit all the test cases. I will not actually run the code here but it is shown for convenience:

```{r, eval=FALSE}
testing <- read.csv("testing.csv")
pred <- predict(modFit, testing)
pred <- as.character(pred)
pml_write_files = function(x){
    n = length(x)
     for(i in 1:n){
         filename = paste0("problem_id_",i,".txt")
         write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
     }
 }
 pml_write_files(pred)
```

The output is a series of 20 text files with the following answers: 

"B" "A" "B" "A" "A" "E" "D" "B" "A" "A" "B" "C" "B" "A" "E" "E" "A" "B" "B" "B"

which are all marked as correct.